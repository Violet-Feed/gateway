# cp /mnt/d/H/Desktop/Violet/gateway/feed-docker-compose.yaml ./
# docker-compose -f feed-docker-compose.yaml up -d xxx
version: "3.8"

networks:
  app_net:
    driver: bridge

services:
  # Redis
  redis:
    image: redis:5.0.14
    container_name: feed-redis
    command: [ "redis-server", "--appendonly", "yes" ]
    ports:
      - "6379:6379"
    volumes:
      - ~/dockerD/mnt/redis/data:/data
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 3s
      retries: 5

  # Kvrocks
  kvrocks:
    image: apache/kvrocks:2.13.0
    container_name: feed-kvrocks
    ports:
      - "6666:6666"
    volumes:
      - ~/dockerD/mnt/kvrocks:/var/lib/kvrocks
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli -h 127.0.0.1 -p 6666 ping | grep PONG || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # RocketMQ namesrv + broker + proxy
  rmq-namesrv:
    image: apache/rocketmq:5.2.0
    container_name: feed-rmq-namesrv
    command: sh mqnamesrv
    ports:
      - "9876:9876"
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "ps | grep -q NamesrvStartup || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 10

  rmq-broker:
    image: apache/rocketmq:5.2.0
    container_name: feed-rmq-broker
    depends_on:
      - rmq-namesrv
    environment:
      - NAMESRV_ADDR=rmq-namesrv:9876
    # 注意：这里的 172.25.99.205 应该是你宿主机的 IP 或者是 rmq-namesrv 的服务名
    # 为了在 compose 内部更稳定，建议使用服务名。但如果你的应用部署在外部，可能需要宿主机 IP。
    command: sh mqbroker -n rmq-namesrv:9876 -c /home/rocketmq/rocketmq-5.2.0/conf/broker.conf
    ports:
      - "10911:10911"
    volumes:
      - ./rocketmq/broker.conf:/home/rocketmq/rocketmq-5.2.0/conf/broker.conf:ro
      - ~/dockerD/mnt/rocketmq/store:/home/rocketmq/store
      - ~/dockerD/mnt/rocketmq/logs:/home/rocketmq/logs
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "ps | grep -q BrokerStartup || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 20

  rmq-proxy:
    image: apache/rocketmq:5.2.0
    container_name: feed-rmq-proxy
    depends_on:
      - rmq-namesrv
      - rmq-broker
    environment:
      - NAMESRV_ADDR=rmq-namesrv:9876
    command: sh mqproxy
    ports:
      - "8080:8080"
      - "8081:8081"
    restart: on-failure
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "ps | grep -q ProxyStartup || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 20

  # Neo4j
  neo4j:
    image: neo4j:5.26-community-bullseye
    container_name: feed-neo4j
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=1g
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - ~/dockerD/mnt/neo4j/data:/data
      - ~/dockerD/mnt/neo4j/logs:/logs
      - ~/dockerD/mnt/neo4j/plugins:/plugins
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:7474 >/dev/null || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 20
      start_period: 30s

  # Elasticsearch
  elasticsearch:
    image: elasticsearch:7.9.2
    container_name: feed-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - ~/dockerD/mnt/es/data:/usr/share/elasticsearch/data
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:9200 >/dev/null || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 20

  # Kafka
  kafka:
    image: apache/kafka:4.0.0
    hostname: kafka
    container_name: feed-kafka
    user: "1000:1000"
    ports:
      - "9092:9092"   # 内部 PLAINTEXT
      - "9093:9093"   # Controller
      - "9094:9094"   # 外部访问
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_LISTENERS: "PLAINTEXT://kafka:9092,CONTROLLER://kafka:9093,OUTSIDE://0.0.0.0:9094"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,OUTSIDE:PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092,OUTSIDE://localhost:9094"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      CLUSTER_ID: "SQiCluster"
      KAFKA_LOG_DIRS: "/tmp/kraft-combined-logs"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks: [ app_net ]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: [ "CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Kafka Connect
  connect:
    image: debezium/connect:2.7.3.Final
    container_name: feed-kafka-connect
    depends_on:
      - kafka
      - mysql
    ports:
      - "8083:8083"
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=connect-cluster
      - CONFIG_STORAGE_TOPIC=_connect_configs
      - OFFSET_STORAGE_TOPIC=_connect_offsets
      - STATUS_STORAGE_TOPIC=_connect_status
      - KEY_CONVERTER=org.apache.kafka.connect.storage.StringConverter
      - VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - VALUE_CONVERTER_SCHEMAS_ENABLE=false
      - KEY_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_REST_ADVERTISED_HOST_NAME=connect
      - CONNECT_LOG4J_LOGGERS=org.reflections=ERROR
      - PLUGIN_PATH=/kafka/connectors
    volumes:
      - ./kafka/connectors:/kafka/connectors:ro
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:8083/connectors || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 20

  # MySQL
  mysql:
    image: mysql:8.0.35-bullseye
    container_name: feed-mysql
    command:
      --default-authentication-plugin=mysql_native_password
      --server-id=1
      --log-bin=mysql-bin
      --binlog-format=ROW
      --binlog-row-image=FULL
      --gtid-mode=ON
      --enforce-gtid-consistency=ON
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_USER=debezium
      - MYSQL_PASSWORD=123456
      - MYSQL_DATABASE=violet
    ports:
      - "3306:3306"
    volumes:
      - ~/dockerD/mnt/mysql/data:/var/lib/mysql
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "mysqladmin ping -h localhost -uroot -proot || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 10

  # Milvus
  milvus:
    image: milvusdb/milvus:v2.6.6
    container_name: feed-milvus
    security_opt:
      - seccomp:unconfined
    environment:
      - ETCD_USE_EMBED=true
      - ETCD_DATA_DIR=/var/lib/milvus/etcd
      - ETCD_CONFIG_PATH=/milvus/configs/embedEtcd.yaml
      - COMMON_STORAGETYPE=local
      - DEPLOY_MODE=STANDALONE
    command: [ "milvus", "run", "standalone" ]
    ports:
      - "19530:19530"   # gRPC
      - "9091:9091"     # /healthz
      - "2379:2379"     # 内嵌 etcd（可选）
    volumes:
      - ~/dockerD/mnt/milvus/data:/var/lib/milvus
      - ./milvus/embedEtcd.yaml:/milvus/configs/embedEtcd.yaml:ro
      - ./milvus/user.yaml:/milvus/configs/user.yaml:ro
    networks: [ app_net ]
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:9091/healthz || exit 1" ]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3

  # Nebula Graph - Metad Service
  metad0:
    image: docker.io/vesoft/nebula-metad:v3.8.0
    container_name: nebula-metad0
    environment:
      USER: root
    command:
      - --meta_server_addrs=metad0:9559
      - --local_ip=metad0
      - --ws_ip=metad0
      - --port=9559
      - --ws_http_port=19559
      - --data_path=/data/meta
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://metad0:19559/status" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9559:9559
      - 19559:19559
      - 19560
    volumes:
      - ~/dockerD/mnt/nebula/data/meta0:/data/meta
      - ~/dockerD/mnt/nebula/logs/meta0:/logs
    networks:
      - app_net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  # Nebula Graph - Storaged Service
  storaged0:
    image: docker.io/vesoft/nebula-storaged:v3.8.0
    container_name: nebula-storaged0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559
      - --local_ip=storaged0
      - --ws_ip=storaged0
      - --port=9779
      - --ws_http_port=19779
      - --data_path=/data/storage
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
    depends_on:
      - metad0
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://storaged0:19779/status" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9779:9779
      - 19779:19779
      - 19780
    volumes:
      - ~/dockerD/mnt/nebula/data/storage0:/data/storage
      - ~/dockerD/mnt/nebula/logs/storage0:/logs
    networks:
      - app_net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  # Nebula Graph - Graphd Service
  graphd:
    image: docker.io/vesoft/nebula-graphd:v3.8.0
    container_name: nebula-graphd
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559
      - --port=9669
      - --local_ip=graphd
      - --ws_ip=graphd
      - --ws_http_port=19669
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
    depends_on:
      - storaged0
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://graphd:19669/status" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9669:9669
      - 19669:19669
      - 19670
    volumes:
      - ~/dockerD/mnt/nebula/logs/graph:/logs
    networks:
      - app_net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  storage-activator:
    # This is just a script to activate storaged for the first time run by calling nebula-console
    # Refer to https://docs.nebula-graph.io/master/4.deployment-and-installation/manage-storage-host/#activate-storaged
    # If you like to call console via docker, run:
    # docker run --rm -ti --network host vesoft/nebula-console:nightly -addr 127.0.0.1 -port 9669 -u root -p nebula
    image: docker.io/vesoft/nebula-console:nightly
    container_name: nebula-storage-activator
    entrypoint: ""
    environment:
      ACTIVATOR_RETRY: ${ACTIVATOR_RETRY:-30}
    command:
      - sh
      - -c
      - |
        for i in `seq 1 $$ACTIVATOR_RETRY`; do
          nebula-console -addr graphd -port 9669 -u root -p nebula -e 'ADD HOSTS "storaged0":9779' 1>/dev/null 2>/dev/null;
          if [[ $$? == 0 ]]; then
            echo "✔️ Storage activated successfully.";
            break;
          else
            output=$$(nebula-console -addr graphd -port 9669 -u root -p nebula -e 'ADD HOSTS "storaged0":9779' 2>&1);
            if echo "$$output" | grep -q "Existed"; then
              echo "✔️ Storage already activated, Exiting...";
              break;
            fi
          fi;
          if [[ $$i -lt $$ACTIVATOR_RETRY ]]; then
            echo "⏳ Attempting to activate storaged, attempt $$i/$$ACTIVATOR_RETRY... It's normal to take some attempts before storaged is ready. Please wait.";
          else
            echo "❌ Failed to activate storaged after $$ACTIVATOR_RETRY attempts. Please check MetaD, StorageD logs. Or restart the storage-activator service to continue retry.";
            echo "ℹ️ Error during storage activation:"
            echo "=============================================================="
            echo "$$output"
            echo "=============================================================="
            break;
          fi;
          sleep 5;
        done && tail -f /dev/null;
    depends_on:
      - graphd
    networks:
      - app_net

  nebula-console:
    image: docker.io/vesoft/nebula-console:v3.8.0
    container_name: nebula-console
    entrypoint: ""
    command: [ "tail", "-f", "/dev/null" ]
    depends_on:
      - graphd
    networks:
      - app_net
    restart: on-failure

  # Nebula Graph Studio - Web UI
  nebula-studio:
    image: vesoft/nebula-graph-studio:v3.10.0
    container_name: nebula-studio
    environment:
      USER: root
    ports:
      - 7001:7001
    networks:
      - app_net
    depends_on:
      - graphd
    restart: on-failure